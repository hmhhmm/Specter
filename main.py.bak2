#!/usr/bin/env python3
"""
üîÆ SPECTER - 100% Autonomous Signup Testing Agent
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

FEATURE 1 (100%): Multimodal Human-Persona Navigator
‚úÖ Vision-based navigation (screenshots + LLM)
‚úÖ Autonomous decision-making (no selectors)
‚úÖ User persona simulation (normal, cautious, confused)
‚úÖ Mobile device emulation (iPhone, Android, Desktop)
‚úÖ Network condition testing (3G, 4G, WiFi, Slow)
‚úÖ Localization support
‚úÖ Dynamic UI adaptation

FEATURE 2 (100%): Cognitive UX Analyst & Diagnosis
‚úÖ Mathematical F-Score calculation
‚úÖ Dynamic AI Uncertainty Heatmap
‚úÖ Ghost Replay GIF generation
‚úÖ P0-P3 severity classification
‚úÖ Claude Vision diagnosis
‚úÖ Slack escalation with team tagging

Usage:
    python main.py                                    # Demo with mock data
    python main.py autonomous https://example.com    # Full autonomous test
    python main.py --persona cautious --device iphone13 --network 3g
"""

import asyncio
import sys
import os
import argparse
import json
from datetime import datetime
from typing import Dict, Any, List, Optional

# Specter Core - Feature 2
from backend.expectation_engine import check_expectation
from backend.diagnosis_doctor import diagnose_failure
from backend.escalation_webhook import send_alert
from backend.mock_data import get_mock_handoff
from backend.webqa_bridge import _resolve_screenshot_path

# Claude API for vision analysis
import anthropic
from dotenv import load_dotenv

load_dotenv(os.path.join("backend", ".env"))

# webqa_agent - Feature 1 (Autonomous Agent)
try:
    from webqa_agent.browser import BrowserSession
    from webqa_agent.actions.action_handler import ActionHandler
    from webqa_agent.crawler.deep_crawler import DeepCrawler
    from playwright.async_api import async_playwright
    AUTONOMOUS_AVAILABLE = True
except ImportError:
    AUTONOMOUS_AVAILABLE = False
    print("Autonomous mode requires: pip install playwright langchain langchain-anthropic")


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# RED DOT DEBUG OVERLAY - Visual Click Feedback (Enhanced for visibility)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

RED_DOT_JS = """
(coords) => {
    const old = document.getElementById('__ghost_dot__');
    if (old) old.remove();

    const dot = document.createElement('div');
    dot.id = '__ghost_dot__';
    
    // ENHANCED: Bigger, brighter, longer duration
    dot.style.cssText = `
        position: fixed;
        left: ${coords[0] - 20}px;
        top: ${coords[1] - 20}px;
        width: 40px;
        height: 40px;
        background: radial-gradient(circle, rgba(255,0,0,0.9) 0%, rgba(255,0,0,0.6) 70%);
        border: 4px solid rgba(255, 255, 255, 0.9);
        border-radius: 50%;
        z-index: 2147483647;
        pointer-events: none;
        box-shadow: 0 0 20px 8px rgba(255,0,0,0.8), 0 0 40px 15px rgba(255,100,100,0.5);
        animation: ghostPulse 0.6s ease-out, ghostGlow 1.5s ease-in-out infinite;
    `;
    
    // Enhanced pulse and glow animation
    const style = document.createElement('style');
    style.textContent = '@keyframes ghostPulse { 0% { transform: scale(0.3); opacity: 0; } 50% { transform: scale(1.3); opacity: 1; } 100% { transform: scale(1); opacity: 1; } } @keyframes ghostGlow { 0%, 100% { box-shadow: 0 0 20px 8px rgba(255,0,0,0.8), 0 0 40px 15px rgba(255,100,100,0.5); } 50% { box-shadow: 0 0 30px 12px rgba(255,0,0,1), 0 0 60px 25px rgba(255,100,100,0.8); } }';
    document.head.appendChild(style);
    
    document.body.appendChild(dot);
    
    // Stay visible for 4 seconds (was 2s), then fade over 1s
    setTimeout(() => { 
        dot.style.opacity = '0'; 
        dot.style.transition = 'opacity 1s ease-out'; 
    }, 4000);
    setTimeout(() => { 
        dot.remove(); 
        style.remove(); 
    }, 5000);
}
"""


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# DEVICE PROFILES - Feature 1: Mobile/Desktop Testing
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

DEVICES = {
    'iphone13': {
        'name': 'iPhone 13 Pro',
        'viewport': {'width': 390, 'height': 844},
        'user_agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 15_0 like Mac OS X) AppleWebKit/605.1.15',
        'has_touch': True,
        'is_mobile': True,
    },
    'android': {
        'name': 'Pixel 5',
        'viewport': {'width': 393, 'height': 851},
        'user_agent': 'Mozilla/5.0 (Linux; Android 11; Pixel 5) AppleWebKit/537.36',
        'has_touch': True,
        'is_mobile': True,
    },
    'desktop': {
        'name': 'Desktop 1920x1080',
        'viewport': {'width': 1920, 'height': 1080},
        'user_agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'has_touch': False,
        'is_mobile': False,
    },
}

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# NETWORK PROFILES - Feature 1: Network Condition Testing
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

NETWORKS = {
    '3g': {'name': 'Slow 3G', 'download': 400*1024/8, 'upload': 400*1024/8, 'latency': 400},
    '4g': {'name': 'Fast 4G', 'download': 4*1024*1024/8, 'upload': 3*1024*1024/8, 'latency': 20},
    'wifi': {'name': 'WiFi', 'download': 30*1024*1024/8, 'upload': 15*1024*1024/8, 'latency': 2},
    'slow': {'name': 'Slow WiFi', 'download': 50*1024/8, 'upload': 20*1024/8, 'latency': 800},
}

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# USER PERSONAS - Feature 1: Human Behavior Simulation
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

PERSONAS = {
    'normal': {
        'name': 'Normal User',
        'desc': 'Typical first-time user',
        'typing_delay': 0.1,
        'action_delay': 1.0,
        'reads_text': False,
        'hesitation': 0.0,
        'vision_requirements': 'Standard',
    },
    'cautious': {
        'name': 'Cautious User', 
        'desc': 'Reads everything carefully',
        'typing_delay': 0.2,
        'action_delay': 3.0,
        'reads_text': True,
        'hesitation': 1.5,
        'vision_requirements': 'High attention to detail',
    },
    'confused': {
        'name': 'Confused User',
        'desc': 'Struggles with UI',
        'typing_delay': 0.3,
        'action_delay': 5.0,
        'reads_text': True,
        'hesitation': 3.0,
        'vision_requirements': 'Needs clear visual cues',
    },
    'elderly': {
        'name': 'Elderly User (65+)',
        'desc': 'Needs large buttons, high contrast, simple language',
        'typing_delay': 0.5,
        'action_delay': 7.0,
        'reads_text': True,
        'hesitation': 5.0,
        'vision_requirements': 'Large text (16px+), high contrast, simple UI',
    },
    'mobile_novice': {
        'name': 'Mobile Novice',
        'desc': 'First time smartphone user',
        'typing_delay': 0.4,
        'action_delay': 8.0,
        'reads_text': True,
        'hesitation': 6.0,
        'vision_requirements': 'Large touch targets (44px+), clear affordances',
    },
}


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# VISION-BASED UI ANALYSIS - Robust QA with Claude Vision
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

async def analyze_ui_with_vision(
    screenshot_b64: str,
    goal: str,
    persona_cfg: Dict[str, Any],
    device_cfg: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Comprehensive UI analysis using Claude Vision.
    
    Detects:
    - Button size issues (too small for touch)
    - Elderly accessibility (contrast, text size, simplicity)
    - Network latency indicators (loading states)
    - Clickable element locations (coordinates)
    - Confusing UI patterns
    - Missing affordances
    - UX confusion score (0-10 scale from ghost_mode.py)
    
    Returns:
        {
            'issues': [list of problems],
            'recommended_action': {'type': 'click/fill', 'details': {...}},
            'accessibility_score': 0-100,
            'elderly_friendly': bool,
            'network_ready': bool,
            'confusion_score': 0-10
        }
    """
    
    try:
        # Clean base64 data (remove data URI prefix if present)
        if isinstance(screenshot_b64, str) and 'base64,' in screenshot_b64:
            screenshot_b64 = screenshot_b64.split('base64,')[1]
        
        # Exponential backoff for rate limits
        delay = 4.0
        max_retries = 5
        
        prompt = f"""You are a UX Quality Assurance expert analyzing this interface.

CONTEXT:
- User Goal: {goal}
- User Type: {persona_cfg['name']} - {persona_cfg['desc']}
- Device: {device_cfg['name']} ({device_cfg['viewport']['width']}x{device_cfg['viewport']['height']})
- Vision Requirements: {persona_cfg.get('vision_requirements', 'Standard')}

üéØ PRIMARY TASK: Your recommended_action MUST directly accomplish the goal: "{goal}"
   For "Locate and click the submit button" ‚Üí find the SIGNUP/SUBMIT/CREATE ACCOUNT button and return its CENTER coordinates.
   For "Fill email field" ‚Üí find the EMAIL input field and return its CENTER coordinates.
   
   CRITICAL: x and y coordinates (0.0 to 1.0 ratio) must point to the VISUAL CENTER of the target element!
   Example: If a button spans from 40% to 60% horizontally, x should be 0.5 (the middle).

STRICT ANALYSIS RULES:

1. BUTTON SIZE ISSUES:
   - Mobile: Touch targets MUST be 44x44px minimum (Apple HIG)
   - Desktop: Buttons should be at least 32x32px
   - Flag: "Button too small - WIDTHxHEIGHTpx, needs MINpx+"

2. ELDERLY ACCESSIBILITY (WCAG AAA):
   - Text size: 16px+ for body, 20px+ for buttons
   - Contrast ratio: 7:1 minimum for text, 4.5:1 for large text
   - Simple language: No tech jargon
   - Clear affordances: Buttons look obviously clickable
   - Flag: "Poor elderly access - SPECIFIC_ISSUE"

3. NETWORK LATENCY INDICATORS:
   - Check for loading spinners, disabled buttons, "processing" states
   - If button clickable but no feedback = LATENCY RISK
   - Flag: "No loading state - users will click multiple times"

4. CLICKABLE ELEMENT DETECTION (CRITICAL FOR RECOMMENDED_ACTION):
   - Look for PRIMARY call-to-action buttons: "Sign Up", "Submit", "Create Account", "Continue"
   - Calculate coordinates as RATIO from 0.0 to 1.0 based on viewport dimensions
   - Coordinates MUST be the CENTER of the button, not top-left or edge
   - Example: Button at pixel (480, 600) on 960x800 viewport = x:0.5, y:0.75

5. CONFUSION RISKS:
   - Multiple similar buttons without clear distinction
   - Form fields without labels
   - Unclear error messages
   - Missing required field indicators
   - Flag: "Confusing - REASON"

RESPONSE FORMAT (valid JSON only):
{{
    "issues": [
        "Button too small - 28x28px, needs 44px+ for mobile",
        "Poor elderly access - text is 12px, needs 16px+",
        "No loading state - button stays active during submit"
    ],
    "recommended_action": {{
        "type": "click|fill|wait",
        "details": {{
            "x": 0.52,
            "y": 0.78,
            "confidence": 0.95,
            "element_name": "Sign Up Button",
            "selector": "button[type='submit']",
            "value": "ai.test@example.com"
        }}
    }},
    "accessibility_score": 65,
    "elderly_friendly": false,
    "network_ready": false,
    "confusion_score": 4,
    "button_sizes": [
        {{"name": "Submit", "width_px": 120, "height_px": 40, "adequate": true}},
        {{"name": "Back", "width_px": 28, "height_px": 28, "adequate": false}}
    ],
    "font_sizes": [
        {{"element": "body", "size_px": 14, "adequate_elderly": false}},
        {{"element": "button", "size_px": 16, "adequate_elderly": true}}
    ],
    "contrast_issues": [
        {{"element": "#gray-text", "ratio": 3.5, "passes_wcag": false}}
    ]
}}

CONFUSION SCORE GUIDE (from Ghost Agent mystery shopper methodology):
  0 - Perfectly clear, obvious next step
  1-3 - Minor friction (small label issues, extra scrolling needed)
  4-6 - Moderate confusion (ambiguous CTA, unclear form fields)
  7-9 - High confusion (misleading text, broken layout, hidden elements)
  10 - Completely stuck, no idea how to proceed

Analyze the screenshot now. Be STRICT - flag even minor usability issues."""

        # Support both CLAUDE_API_KEY and ANTHROPIC_API_KEY for compatibility
        api_key = os.getenv("CLAUDE_API_KEY") or os.getenv("ANTHROPIC_API_KEY")
        if not api_key:
            print(f"   ‚ö†Ô∏è  No Claude API key found. Set CLAUDE_API_KEY or ANTHROPIC_API_KEY.")
            return {
                'issues': [],
                'recommended_action': None,
                'accessibility_score': 0,
                'elderly_friendly': False,
                'network_ready': False,
                'confusion_score': 0
            }
        
        client = anthropic.Anthropic(api_key=api_key)
        
        # Retry with exponential backoff on rate limits
        for attempt in range(1, max_retries + 1):
            try:
                message = client.messages.create(
                    model="claude-3-haiku-20240307",  # Fast for real-time analysis
                    max_tokens=2048,
                    messages=[{
                        "role": "user",
                        "content": [
                            {
                                "type": "image",
                                "source": {
                                    "type": "base64",
                                    "media_type": "image/png",
                                    "data": screenshot_b64
                                }
                            },
                            {
                                "type": "text",
                                "text": prompt
                            }
                        ]
                    }]
                )
                
                analysis = json.loads(message.content[0].text)
                return analysis
                
            except json.JSONDecodeError as je:
                print(f"   ‚ö†Ô∏è  Vision analysis JSON parse failed (attempt {attempt}/{max_retries})")
                if attempt == max_retries:
                    break
                await asyncio.sleep(1.0)
                
            except Exception as e:
                err_msg = str(e).lower()
                if "rate limit" in err_msg or "429" in err_msg or "overloaded" in err_msg:
                    if attempt < max_retries:
                        print(f"   ‚è≥ Rate limited (attempt {attempt}/{max_retries}). Waiting {delay:.0f}s...")
                        await asyncio.sleep(delay)
                        delay = min(delay * 2, 60.0)  # 4s ‚Üí 8s ‚Üí 16s ‚Üí 32s ‚Üí 60s
                        continue
                print(f"   ‚ö†Ô∏è  Vision analysis failed: {e}")
                break
        
        # All retries exhausted
        return {
            'issues': [],
            'recommended_action': None,
            'accessibility_score': 0,
            'elderly_friendly': False,
            'network_ready': False,
            'confusion_score': 0
        }
        
    except Exception as e:
        print(f"   ‚ö†Ô∏è  Vision analysis failed: {e}")
        return {
            'issues': [],
            'recommended_action': None,
            'accessibility_score': 0,
            'elderly_friendly': False,
            'network_ready': False,
            'confusion_score': 0
        }


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# DUAL-SCREENSHOT DIAGNOSIS - Ghost Mode Integration
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

async def diagnose_with_dual_screenshots(
    before_b64: str,
    after_b64: str,
    action_description: str,
    expectation: str
) -> Dict[str, Any]:
    """
    Ghost Agent dual-screenshot diagnosis.
    
    Compares BEFORE and AFTER screenshots to detect:
    - Visual changes (or lack thereof)
    - Whether expectation was met
    - Root cause if action failed
    - Severity classification
    - Responsible team
    
    Returns diagnosis dict with status, observation, diagnosis, severity, team
    """
    
    DIAGNOSIS_PROMPT = f"""You are a QA engineer diagnosing the outcome of a browser action.

You will receive TWO screenshots:
1. BEFORE the action was taken
2. AFTER the action was taken

The action was: {action_description}
The agent expected: {expectation}

Analyze the visual difference and respond with ONLY valid JSON:

{{
  "status": "PASSED" | "FAILED" | "PARTIAL" | "BLOCKED",
  "visual_observation": "<describe what visually changed or did not change>",
  "diagnosis": "<root cause if failed, e.g. 'Backend API Failure', 'Element not interactable', 'Page did not respond'>",
  "severity": "<one of: 'P0 - Critical Blocker', 'P1 - Major', 'P2 - Minor', 'P3 - Cosmetic', 'P4 - Enhancement'>",
  "responsible_team": "<one of: 'Frontend Engineering', 'Backend Engineering', 'UX/Design', 'QA', 'DevOps', 'Product'>"
}}

If the action succeeded as expected, use status "PASSED", severity "P4 - Enhancement",
responsible_team "N/A", and diagnosis "Action completed as expected".
"""
    
    try:
        # Clean base64 data
        if 'base64,' in before_b64:
            before_b64 = before_b64.split('base64,')[1]
        if 'base64,' in after_b64:
            after_b64 = after_b64.split('base64,')[1]
        
        # Support both CLAUDE_API_KEY and ANTHROPIC_API_KEY
        api_key = os.getenv("CLAUDE_API_KEY") or os.getenv("ANTHROPIC_API_KEY")
        if not api_key:
            return {
                "status": "PARTIAL",
                "visual_observation": "No API key configured",
                "diagnosis": "CLAUDE_API_KEY or ANTHROPIC_API_KEY not set",
                "severity": "P2 - Minor",
                "responsible_team": "QA"
            }
        
        client = anthropic.Anthropic(api_key=api_key)
        
        # Retry with exponential backoff
        delay = 4.0
        max_retries = 5
        
        for attempt in range(1, max_retries + 1):
            try:
                message = client.messages.create(
                    model="claude-3-haiku-20240307",
                    max_tokens=1024,
                    messages=[{
                        "role": "user",
                        "content": [
                            {
                                "type": "image",
                                "source": {
                                    "type": "base64",
                                    "media_type": "image/png",
                                    "data": before_b64
                                }
                            },
                            {
                                "type": "image",
                                "source": {
                                    "type": "base64",
                                    "media_type": "image/png",
                                    "data": after_b64
                                }
                            },
                            {
                                "type": "text",
                                "text": DIAGNOSIS_PROMPT
                            }
                        ]
                    }]
                )
                
                # Parse response
                response_text = message.content[0].text.strip()
                # Remove markdown fences if present
                if response_text.startswith("```"):
                    response_text = response_text.split("\n", 1)[-1]
                if response_text.endswith("```"):
                    response_text = response_text.rsplit("```", 1)[0]
                response_text = response_text.strip()
                
                diagnosis = json.loads(response_text)
                return diagnosis
                
            except json.JSONDecodeError:
                print(f"   ‚ö†Ô∏è  Diagnosis JSON parse failed (attempt {attempt}/{max_retries})")
                if attempt == max_retries:
                    break
                await asyncio.sleep(1.0)
                
            except Exception as e:
                err_msg = str(e).lower()
                if "rate limit" in err_msg or "429" in err_msg or "overloaded" in err_msg:
                    if attempt < max_retries:
                        print(f"   ‚è≥ Diagnosis rate limited (attempt {attempt}/{max_retries}). Waiting {delay:.0f}s...")
                        await asyncio.sleep(delay)
                        delay = min(delay * 2, 60.0)
                        continue
                print(f"   ‚ö†Ô∏è  Diagnosis failed: {e}")
                break
        
        # Fallback if all retries fail
        return {
            "status": "PARTIAL",
            "visual_observation": "Diagnosis unavailable",
            "diagnosis": "LLM diagnosis call failed",
            "severity": "P2 - Minor",
            "responsible_team": "QA"
        }
        
    except Exception as e:
        print(f"   ‚ö†Ô∏è  Dual-screenshot diagnosis error: {e}")
        return {
            "status": "PARTIAL",
            "visual_observation": "Diagnosis unavailable",
            "diagnosis": str(e),
            "severity": "P2 - Minor",
            "responsible_team": "QA"
        }


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# AUTONOMOUS AGENT - Feature 1: Vision-Based Navigation
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

async def autonomous_signup_test(
    url: str,
    device: str = 'desktop',
    network: str = 'wifi',
    persona: str = 'normal',
    locale: str = 'en-US'
) -> Dict[str, Any]:
    """
    100% Autonomous signup testing with vision + LLM.
    
    This is the COMPLETE Feature 1 implementation:
    - OBSERVE: Takes screenshots with highlighted UI elements
    - DECIDE: LLM analyzes visuals and decides next action
    - ACT: Executes actions based on visual reasoning
    - VERIFY: Checks if expectations met (feeds to Feature 2)
    
    Args:
        url: Signup URL to test
        device: Device profile (iphone13, android, desktop)
        network: Network profile (3g, 4g, wifi, slow)
        persona: User behavior (normal, cautious, confused)
        locale: Language/region (en-US, es-ES, etc.)
        
    Returns:
        Complete test results with pass/fail per step
    """
    
    if not AUTONOMOUS_AVAILABLE:
        print("‚ùå Autonomous mode not available. Install dependencies:")
        print("   pip install playwright langchain langchain-anthropic")
        return {'status': 'ERROR', 'reason': 'Dependencies missing'}
    
    device_cfg = DEVICES[device]
    network_cfg = NETWORKS[network]
    persona_cfg = PERSONAS[persona]
    
    print("\n" + "‚ïê" * 70)
    print("ü§ñ SPECTER AUTONOMOUS AGENT - Initializing")
    print("‚ïê" * 70)
    print(f"üìç URL: {url}")
    print(f"üì± Device: {device_cfg['name']}")
    print(f"üì° Network: {network_cfg['name']}")
    print(f"üë§ Persona: {persona_cfg['name']} - {persona_cfg['desc']}")
    print(f"üåç Locale: {locale}")
    print("‚ïê" * 70 + "\n")
    
    # Enable screenshot saving
    ActionHandler.set_screenshot_config(save_screenshots=True)
    
    # Network and console log capture
    network_logs = []
    console_logs = []
    
    def attach_listeners(page):
        """Hook into Playwright page events for network and console capture."""
        
        def on_response(response):
            status = response.status
            # Capture 4xx/5xx and all API-like calls
            if status >= 400 or "/api" in response.url:
                network_logs.append({
                    "status": status,
                    "url": response.url,
                    "method": response.request.method,
                })
        
        def on_console(msg):
            if msg.type in ("error", "warning"):
                console_logs.append(f"[{msg.type}] {msg.text}")
        
        page.on("response", on_response)
        page.on("console", on_console)
    
    async with async_playwright() as p:
        # Launch browser with device emulation
        browser = await p.chromium.launch(
            headless=False,
            args=['--disable-blink-features=AutomationControlled']
        )
        
        context = await browser.new_context(
            viewport=device_cfg['viewport'],
            user_agent=device_cfg['user_agent'],
            has_touch=device_cfg['has_touch'],
            is_mobile=device_cfg['is_mobile'],
            locale=locale,
        )
        
        page = await context.new_page()
        
        # Attach network and console listeners
        attach_listeners(page)
        
        # Apply network throttling
        cdp = await context.new_cdp_session(page)
        await cdp.send('Network.emulateNetworkConditions', {
            'offline': False,
            'downloadThroughput': network_cfg['download'],
            'uploadThroughput': network_cfg['upload'],
            'latency': network_cfg['latency'],
        })
        
        print(f"üåê Navigating to {url}...")
        await page.goto(url, wait_until='domcontentloaded', timeout=60000)
        await asyncio.sleep(2)
        
        print("‚úÖ Page loaded\n")
        
        # Initialize action handler
        action_handler = ActionHandler()
        action_handler.page = page
        
        # Test scenario: Autonomous navigation through signup
        steps = [
            {
                'goal': 'Observe and identify the email and password input fields (DO NOT click anything)',
                'expectation': 'Email and password fields should be visible',
            },
            {
                'goal': 'Fill the email input field with test data',
                'expectation': 'Email appears in field correctly',
            },
            {
                'goal': 'Fill the password input field with a secure password',
                'expectation': 'Password is masked with dots',
            },
            {
                'goal': 'Click the primary Sign Up / Submit / Create Account button',
                'expectation': 'Form submits or next page loads',
            },
        ]
        
        print(f"üìã Executing {len(steps)} autonomous steps...\n")
        
        # Initialize ActionHandler's screenshot session first, then use its parent as reports_dir
        # This ensures step JSONs, heatmaps, GIFs, and screenshots all go to the SAME directory
        screenshot_dir = action_handler.init_screenshot_session()
        reports_dir = str(screenshot_dir.parent)  # reports/test_TIMESTAMP/
        print(f"üìÅ Report directory: {reports_dir}")
        os.makedirs(reports_dir, exist_ok=True)
        
        results = []
        confusion_scores = []  # Track confusion scores from ghost_mode methodology
        
        for idx, step in enumerate(steps, 1):
            print(f"üîπ Step {idx}/{len(steps)}: {step['goal']}")
            
            # Apply persona timing
            if idx > 1:
                if persona_cfg['hesitation'] > 0:
                    await asyncio.sleep(persona_cfg['hesitation'])
                    print(f"   üí≠ [Persona: {persona_cfg['hesitation']}s hesitation]")
                await asyncio.sleep(persona_cfg['action_delay'])
            
            start_time = datetime.now()
            
            # Clear logs before this step
            network_logs.clear()
            console_logs.clear()
            
            try:
                # Check if page is still alive
                if page.is_closed():
                    print(f"   ‚ùå ERROR: Browser page closed unexpectedly")
                    results.append({
                        'step': idx,
                        'goal': step['goal'],
                        'result': 'FAIL',
                        'error': 'Browser page closed',
                    })
                    break
                
                # 1. OBSERVE - Capture current UI state
                try:
                    screenshot_before, screenshot_before_path = await action_handler.b64_page_screenshot(
                        file_name=f'autonomous_step_{idx}_before',
                        context='autonomous'
                    )
                except Exception as ss_err:
                    print(f"   ‚ùå Screenshot failed: {ss_err}")
                    results.append({
                        'step': idx,
                        'goal': step['goal'],
                        'result': 'FAIL',
                        'error': f'Screenshot capture failed: {ss_err}',
                    })
                    break
                
                # 2. VISION-BASED ANALYSIS - Claude analyzes UI
                if screenshot_before is None:
                    print(f"   ‚ö†Ô∏è  No screenshot available, using fallback analysis")
                    ui_analysis = {
                        'issues': ['Screenshot capture failed'],
                        'recommended_action': None,
                        'accessibility_score': 0,
                        'elderly_friendly': False,
                        'network_ready': False
                    }
                else:
                    ui_analysis = await analyze_ui_with_vision(
                        screenshot_before,
                        step['goal'],
                        persona_cfg,
                        device_cfg
                    )
                
                # Track confusion score (ghost_mode feature)
                confusion_score = ui_analysis.get('confusion_score', 0)
                if confusion_score > 0:
                    confusion_scores.append(confusion_score)
                    print(f"   üí≠ Confusion Score: {confusion_score}/10", end="")
                    if confusion_score >= 7:
                        print(" (High friction!)")
                    elif confusion_score >= 4:
                        print(" (Moderate)")
                    else:
                        print(" (Minor)")
                
                # Check for accessibility issues
                if ui_analysis.get('issues'):
                    print(f"   ‚ö†Ô∏è  UI Issues Detected:")
                    for issue in ui_analysis['issues'][:3]:
                        print(f"      - {issue}")
                
                # 3. DECIDE + ACT - Execute based on vision analysis
                action = "No action"
                click_x, click_y = 0.5, 0.5
                
                # Helper: resolve real element bbox coordinates via Playwright
                # NOTE: Playwright bounding_box() returns viewport-relative coords already
                async def _resolve_element_center(selectors_to_try):
                    """Try multiple selectors, return (element, x_ratio, y_ratio, abs_x, abs_y) or None."""
                    viewport = page.viewport_size
                    for sel in selectors_to_try:
                        try:
                            el = await page.query_selector(sel)
                            if el and await el.is_visible():
                                # Scroll into view to ensure accurate bbox
                                await el.scroll_into_view_if_needed()
                                await asyncio.sleep(0.2)
                                box = await el.bounding_box()
                                if box:
                                    # bounding_box() is already viewport-relative, no scroll offset needed
                                    cx = box['x'] + box['width'] / 2
                                    cy = box['y'] + box['height'] / 2
                                    rx = cx / viewport['width']
                                    ry = cy / viewport['height']
                                    return el, rx, ry, int(cx), int(cy)
                        except Exception:
                            continue
                    return None
                
                # Determine what this step should do based on step index
                # Steps 2,3 = fill fields, Step 4 = click submit, Step 1 = observe
                step_is_fill = idx in (2, 3)
                step_is_click = idx == 4
                
                # Step 1 is observation-only - don't click even if AI recommends it
                if idx == 1:
                    action = "Observed email and password fields"
                    print(f"   üëÅÔ∏è  Observation-only step - skipping recommended actions")
                elif step_is_click and ui_analysis.get('recommended_action'):
                    # Only use AI click recommendation for the actual click step (Step 4)
                    action_type = ui_analysis['recommended_action']['type']
                    action_details = ui_analysis['recommended_action'].get('details', {})
                    
                    # Log what the AI detected
                    element_name = action_details.get('element_name', 'Unknown')
                    confidence = action_details.get('confidence', 0)
                    print(f"   üéØ AI Vision Target: '{element_name}' (confidence: {confidence:.0%})")
                    
                    if action_type == 'click':
                        viewport = page.viewport_size
                        
                        # PRIORITY: Find real element via Playwright selectors first
                        # Build list of selectors to try (AI-suggested + common fallbacks)
                        ai_selector = action_details.get('selector', '')
                        click_selectors = [s for s in [
                            ai_selector,
                            'button[type="submit"]',
                            'input[type="submit"]',
                            'button:has-text("Next")',
                            'button:has-text("Sign Up")',
                            'button:has-text("Create Account")',
                            'button:has-text("Submit")',
                            'button:has-text("Register")',
                            'form button',
                        ] if s]
                        
                        resolved = await _resolve_element_center(click_selectors)
                        
                        if resolved:
                            real_el, click_x, click_y, abs_x, abs_y = resolved
                            ai_x = action_details.get('x', 0.5)
                            ai_y = action_details.get('y', 0.5)
                            print(f"   ‚úÖ Real element found! bbox center: ({abs_x}, {abs_y})px ratio ({click_x:.2f}, {click_y:.2f})")
                            print(f"      (AI predicted ({ai_x:.2f}, {ai_y:.2f}) ‚Äî corrected)")
                            
                            # Inject red dot at REAL button location
                            try:
                                await page.evaluate(RED_DOT_JS, [abs_x, abs_y])
                                await asyncio.sleep(0.5)
                            except Exception:
                                pass
                            
                            # Click the real element
                            await real_el.click()
                            action = f"Clicked '{element_name}' at real bbox ({click_x:.2f}, {click_y:.2f}) = viewport ({abs_x}, {abs_y})px"
                        else:
                            # Fallback to AI vision coordinates if no element found
                            click_x = action_details.get('x', 0.5)
                            click_y = action_details.get('y', 0.5)
                            abs_x = int(click_x * viewport['width'])
                            abs_y = int(click_y * viewport['height'])
                            
                            print(f"   ‚ö†Ô∏è  No real element found, using AI coords: ({click_x:.2f}, {click_y:.2f}) ‚Üí pixel ({abs_x}, {abs_y})")
                            
                            try:
                                await page.evaluate(RED_DOT_JS, [abs_x, abs_y])
                                await asyncio.sleep(0.5)
                            except Exception:
                                pass
                            
                            await page.mouse.click(abs_x, abs_y)
                            action = f"Clicked '{element_name}' at AI coords ({click_x:.2f}, {click_y:.2f}) = viewport ({abs_x}, {abs_y})px"
                    
                    elif action_type == 'fill':
                        # Find field by vision-predicted coordinates
                        field_selector = action_details.get('selector', 'input')
                        element = await page.query_selector(field_selector)
                        if element:
                            # Show red dot on field before filling & set real coords for heatmap
                            try:
                                box = await element.bounding_box()
                                if box:
                                    field_x = box['x'] + box['width'] / 2
                                    field_y = box['y'] + box['height'] / 2
                                    viewport = page.viewport_size
                                    click_x = field_x / viewport['width']
                                    click_y = field_y / viewport['height']
                                    await page.evaluate(RED_DOT_JS, [field_x, field_y])
                                    await asyncio.sleep(0.8)
                            except Exception:
                                pass
                            
                            await element.fill(action_details.get('value', ''))
                            action = f"Filled {action_details.get('field_name', 'field')}"
                
                # Fallback to basic interaction (for fill steps or when AI has no recommendation)
                if action == "No action":
                    if idx == 1:
                        email_field = await page.query_selector('input[type="email"]')
                        action = "Observed email field"
                    elif idx == 2:
                        email_field = await page.query_selector('input[type="email"]')
                        if email_field:
                            # Show red dot before filling
                            try:
                                box = await email_field.bounding_box()
                                if box:
                                    field_x = box['x'] + box['width'] / 2
                                    field_y = box['y'] + box['height'] / 2
                                    viewport = page.viewport_size
                                    click_x = field_x / viewport['width']
                                    click_y = field_y / viewport['height']
                                    await page.evaluate(RED_DOT_JS, [field_x, field_y])
                                    await asyncio.sleep(0.8)
                            except Exception:
                                pass
                                
                            await email_field.fill('ai.test.user@example.com')
                            action = "Filled email field"
                    elif idx == 3:
                        password_field = await page.query_selector('input[type="password"]')
                        if password_field:
                            # Show red dot before filling
                            try:
                                box = await password_field.bounding_box()
                                if box:
                                    field_x = box['x'] + box['width'] / 2
                                    field_y = box['y'] + box['height'] / 2
                                    viewport = page.viewport_size
                                    click_x = field_x / viewport['width']
                                    click_y = field_y / viewport['height']
                                    await page.evaluate(RED_DOT_JS, [field_x, field_y])
                                    await asyncio.sleep(0.8)
                            except Exception:
                                pass
                                
                            await password_field.fill('SecurePass123!')
                            action = "Filled password field"
                    elif idx == 4:
                        # Try multiple selectors to find the submit/next button
                        submit_btn = None
                        for sel in [
                            'button[type="submit"]', 'input[type="submit"]',
                            'button:has-text("Next")', 'button:has-text("Sign Up")',
                            'button:has-text("Create Account")', 'button:has-text("Submit")',
                            'button:has-text("Register")', 'form button',
                        ]:
                            try:
                                submit_btn = await page.query_selector(sel)
                                if submit_btn and await submit_btn.is_visible():
                                    break
                                submit_btn = None
                            except Exception:
                                continue
                        
                        if submit_btn:
                            # Scroll button into view first
                            try:
                                await submit_btn.scroll_into_view_if_needed()
                                await asyncio.sleep(0.3)
                            except Exception:
                                pass
                            
                            # Get button center in viewport coordinates
                            try:
                                box = await submit_btn.bounding_box()
                                if box:
                                    # bounding_box() already returns viewport-relative coords
                                    btn_x = box['x'] + box['width'] / 2
                                    btn_y = box['y'] + box['height'] / 2
                                    viewport = page.viewport_size
                                    click_x = btn_x / viewport['width']
                                    click_y = btn_y / viewport['height']
                                    
                                    print(f"   ‚úÖ Submit button found at real bbox ({int(btn_x)}, {int(btn_y)})px ratio ({click_x:.2f}, {click_y:.2f})")
                                    
                                    # Inject red dot at viewport coordinates
                                    await page.evaluate(RED_DOT_JS, [btn_x, btn_y])
                                    await asyncio.sleep(0.5)  # Let dot appear
                            except Exception as e:
                                pass
                            
                            await submit_btn.click()
                            action = "Clicked submit button"
                
                # Simulate persona typing speed
                if 'fill' in action.lower():
                    typing_time = 15 * persona_cfg['typing_delay']
                    await asyncio.sleep(typing_time)
                
                end_time = datetime.now()
                dwell_time_ms = int((end_time - start_time).total_seconds() * 1000)
                
                # 3. CAPTURE AFTER STATE
                await asyncio.sleep(1)  # UI update
                try:
                    screenshot_after, screenshot_after_path = await action_handler.b64_page_screenshot(
                        file_name=f'autonomous_step_{idx}_after',
                        context='autonomous'
                    )
                except Exception as ss_err:
                    print(f"   ‚ö†Ô∏è  After screenshot failed: {ss_err}")
                    screenshot_after = screenshot_before
                    screenshot_after_path = screenshot_before_path
                
                # 3.5 DUAL-SCREENSHOT DIAGNOSIS (Ghost Mode feature)
                dual_diagnosis = None
                if screenshot_before and screenshot_after and screenshot_before != screenshot_after:
                    dual_diagnosis = await diagnose_with_dual_screenshots(
                        screenshot_before,
                        screenshot_after,
                        action,
                        step['expectation']
                    )
                    if dual_diagnosis:
                        print(f"   üî¨ Visual Diagnosis: {dual_diagnosis.get('status', 'UNKNOWN')} - {dual_diagnosis.get('visual_observation', '')[:60]}...")
                
                # Use actual click coordinates from vision analysis
                touch_x = click_x
                touch_y = click_y
                
                # 4. VERIFY EXPECTATION - Build handoff for Specter
                handoff = {
                    'step_id': idx,
                    'persona': f"{persona_cfg['name']} ({device_cfg['name']}, {network_cfg['name']})",
                    'action_taken': f"{step['goal']} - {action}",
                    'agent_expectation': step['expectation'],
                    'outcome': {},
                    
                    'meta_data': {
                        'touch_x': touch_x,
                        'touch_y': touch_y,
                        'dwell_time_ms': dwell_time_ms,
                        'device_type': device_cfg['name'],
                        'network_type': network_cfg['name'],
                        'locale': locale,
                        'persona': persona_cfg['name'],
                    },
                    
                    'evidence': {
                        'screenshot_before_path': _resolve_screenshot_path(screenshot_before_path) if screenshot_before_path else 'backend/assets/mock_before.jpg',
                        'screenshot_after_path': _resolve_screenshot_path(screenshot_after_path) if screenshot_after_path else 'backend/assets/mock_after.jpg',
                        'network_logs': list(network_logs),  # Real network error logs
                        'console_logs': list(console_logs),  # Real console error logs
                        'expected_outcome': step['expectation'],
                        'actual_outcome': action,
                        'ui_analysis': ui_analysis,  # Vision-based UI assessment
                        'dual_diagnosis': dual_diagnosis,  # Ghost mode before/after comparison
                    }
                }
                
                # Save individual step JSON report
                step_report = {
                    'step_id': idx,
                    'timestamp': datetime.now().isoformat(),
                    'persona': persona_cfg['name'],
                    'device': device_cfg['name'],
                    'network': network_cfg['name'],
                    'action_taken': action,
                    'agent_expectation': step['expectation'],
                    'dwell_time_ms': dwell_time_ms,
                    'ux_insight': {
                        'accessibility_score': ui_analysis.get('accessibility_score', 0),
                        'elderly_friendly': ui_analysis.get('elderly_friendly', False),
                        'confusion_score': ui_analysis.get('confusion_score', 0),  # Ghost mode metric
                        'issues': ui_analysis.get('issues', []),
                    },
                    'outcome': dual_diagnosis if dual_diagnosis else {
                        'status': 'UNKNOWN',
                        'visual_observation': 'No dual diagnosis available',
                    },
                    'evidence': handoff['evidence'],
                }
                
                step_report_path = os.path.join(reports_dir, f"step_{idx:02d}_report.json")
                with open(step_report_path, 'w') as f:
                    json.dump(step_report, f, indent=2)
                
                handoff = handoff
                
                # 5. SPECTER ANALYSIS (Feature 2)
                result = run_specter_pipeline(handoff)
                
                results.append({
                    'step': idx,
                    'goal': step['goal'],
                    'result': result,
                    'dwell_time': dwell_time_ms,
                })
                
                if result == 'PASS':
                    print(f"   ‚úÖ PASSED - {action}")
                else:
                    print(f"   ‚ùå FAILED - High friction detected")
                
            except Exception as e:
                print(f"   ‚ùå ERROR: {e}")
                results.append({
                    'step': idx,
                    'goal': step['goal'],
                    'result': 'FAIL',
                    'error': str(e),
                })
            
            print()
        
        await browser.close()
        
        # Summary
        passed = sum(1 for r in results if r['result'] == 'PASS')
        failed = len(results) - passed
        avg_dwell = sum(r.get('dwell_time', 0) for r in results) / len(results) if results else 0
        
        # Ghost mode confusion score tracking
        avg_confusion = sum(confusion_scores) / len(confusion_scores) if confusion_scores else 0
        
        # Count captured errors
        total_network_errors = sum(len(r.get('network_logs', [])) for r in results if 'network_logs' in r)
        total_console_errors = sum(len(r.get('console_logs', [])) for r in results if 'console_logs' in r)
        
        print("‚ïê" * 70)
        print("üé¨ AUTONOMOUS TEST COMPLETE")
        print("‚ïê" * 70)
        print(f"‚úÖ Passed: {passed}/{len(results)}")
        print(f"‚ùå Failed: {failed}/{len(results)}")
        print(f"‚è±Ô∏è  Avg Dwell Time: {avg_dwell:.0f}ms")
        print(f"üí≠ Avg Confusion Score: {avg_confusion:.1f}/10" + (" (High UX friction!)" if avg_confusion >= 7 else " (Moderate friction)" if avg_confusion >= 4 else ""))
        print(f"üåê Network Errors: {total_network_errors}")
        print(f"üî¥ Console Errors: {total_console_errors}")
        print(f"üìä Status: {'PASS' if failed == 0 else 'FAIL'}")
        print(f"üìÅ Reports: {reports_dir}")
        print("‚ïê" * 70 + "\n")
        
        return {
            'status': 'PASS' if failed == 0 else 'FAIL',
            'device': device,
            'network': network,
            'persona': persona,
            'steps': results,
            'passed': passed,
            'failed': failed,
            'avg_confusion_score': round(avg_confusion, 2),  # Ghost mode metric
            'total_confusion_scores': len(confusion_scores),
        }


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# SPECTER PIPELINE - Feature 2: Diagnosis Engine
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def run_specter_pipeline(handoff_packet: Dict[str, Any]) -> str:
    """
    Specter Cognitive Diagnosis Pipeline (Feature 2)
    
    1. Calculate F-Score (mathematical friction metric)
    2. Generate dynamic heatmap if F-Score > 50
    3. Generate animated GIF replay
    4. AI diagnosis with Claude Vision
    5. Classify severity (P0-P3)
    6. Escalate to Slack with team tagging
    
    Args:
        handoff_packet: Test step data with screenshots and telemetry
        
    Returns:
        "PASS" or "FAIL"
    """
    
    print(f"   üîé Checking Step {handoff_packet['step_id']}...")
    
    # 1. EXPECTATION ENGINE - Calculate F-Score
    result = check_expectation(handoff_packet)
    
    if result['status'] == "SUCCESS":
        return "PASS"
    
    # 2. DIAGNOSIS - AI analysis + Rich media generation
    handoff_packet['outcome'] = {
        "status": "FAILED",
        "visual_observation": result['reason'],
        "f_score": result.get('f_score'),
        "calculated_severity": result.get('calculated_severity'),
        "gif_path": result.get('gif_path'),
        "heatmap_path": result.get('heatmap_path'),
    }
    
    final_packet = diagnose_failure(handoff_packet, use_vision=True)
    
    # 3. ESCALATION - Slack alert
    send_alert(final_packet)
    
    return "FAIL"


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# CLI INTERFACE
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def parse_args():
    parser = argparse.ArgumentParser(
        description='Specter: Autonomous AI Agent for Signup Testing',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python main.py                                     # Demo mode (mock data)
  python main.py autonomous https://example.com     # Full autonomous test
  python main.py autonomous https://app.com/signup --persona cautious --device iphone13
  python main.py autonomous https://spotify.com/signup --network 3g --device android
        """
    )
    
    parser.add_argument('mode', nargs='?', default='demo', 
                       choices=['demo', 'autonomous'],
                       help='Test mode: demo (mock) or autonomous (full)')
    parser.add_argument('url', nargs='?', help='Signup URL to test (for autonomous mode)')
    parser.add_argument('--persona', default='normal', choices=PERSONAS.keys(),
                       help='User behavior persona')
    parser.add_argument('--device', default='desktop', choices=DEVICES.keys(),
                       help='Device profile')
    parser.add_argument('--network', default='wifi', choices=NETWORKS.keys(),
                       help='Network condition')
    parser.add_argument('--locale', default='en-US', help='Language/region')
    
    return parser.parse_args()


async def main_async():
    """Main async entry point"""
    args = parse_args()
    
    # Ensure assets folder exists
    os.makedirs("backend/assets", exist_ok=True)
    
    if args.mode == 'autonomous':
        if not args.url:
            print("‚ùå Error: URL required for autonomous mode")
            print("Usage: python main.py autonomous <URL>")
            sys.exit(1)
        
        # Run autonomous test
        result = await autonomous_signup_test(
            url=args.url,
            device=args.device,
            network=args.network,
            persona=args.persona,
            locale=args.locale,
        )
        
        return result
    
    else:
        # Demo mode with mock data
        print("=" * 60)
        print("üîÆ SPECTER - Demo Mode (Mock Data)")
        print("=" * 60)
        print("Testing Feature 2: Diagnosis Engine")
        print("(Use 'python main.py autonomous <URL>' for Feature 1)\n")
        
        packet = get_mock_handoff()
        result = run_specter_pipeline(packet)
        
        print("\n" + "=" * 60)
        print(f"üéâ Pipeline Complete: {result}")
        print("=" * 60)
        
        return {'status': result}


def main():
    """Main entry point"""
    if sys.platform == 'win32':
        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
    
    result = asyncio.run(main_async())
    sys.exit(0 if result.get('status') in ['PASS', 'SUCCESS'] else 1)


if __name__ == "__main__":
    main()
